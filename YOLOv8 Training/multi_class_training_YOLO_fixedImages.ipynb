{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import logging\n",
    "from typing import Dict, List, Set, Tuple  # Added Tuple here\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set GPU memory allocation to 80%\n",
    "torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "\n",
    "def set_all_seeds(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def create_yaml_file(train_dir: str, \n",
    "                    val_dir: str, \n",
    "                    yaml_path: str, \n",
    "                    num_classes: int, \n",
    "                    class_names: List[str]):\n",
    "    \"\"\"\n",
    "    Create YAML configuration file for YOLOv8\n",
    "    Args:\n",
    "        train_dir: Path to training images directory\n",
    "        val_dir: Path to validation images directory\n",
    "        yaml_path: Output path for YAML file\n",
    "        num_classes: Number of classes\n",
    "        class_names: List of class names\n",
    "    \"\"\"\n",
    "    train_dir = os.path.abspath(train_dir)\n",
    "    val_dir = os.path.abspath(val_dir)\n",
    "    \n",
    "    yaml_content = {\n",
    "        'train': os.path.join(train_dir, 'images'),\n",
    "        'val': os.path.join(val_dir, 'images'),\n",
    "        'nc': num_classes,\n",
    "        'names': class_names\n",
    "    }\n",
    "    \n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(yaml_content, f)\n",
    "    \n",
    "    logging.info(f\"Created YAML configuration file at {yaml_path}\")\n",
    "\n",
    "def get_background_images(train_image_count: int, \n",
    "                         train_annotations: pd.DataFrame, \n",
    "                         background_dir: str, \n",
    "                         background_percentage: float = 0,\n",
    "                         random_state: int = 42) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get random background images that don't contain any annotations\n",
    "    Args:\n",
    "        train_image_count: Number of training images (not boxes)\n",
    "        train_annotations: Full annotations DataFrame\n",
    "        background_dir: Directory containing background images\n",
    "        background_percentage: Percentage of training images to use as background\n",
    "        random_state: Random seed\n",
    "    Returns:\n",
    "        List of selected background image IDs\n",
    "    \"\"\"\n",
    "    # Calculate number of background images needed\n",
    "    num_background = int(train_image_count * (background_percentage / 100))\n",
    "    \n",
    "    # Get all image IDs that contain any annotations\n",
    "    annotated_images = set(train_annotations['ImageID'].unique())\n",
    "    \n",
    "    # Get all available background images\n",
    "    try:\n",
    "        background_files = [f[:-4] for f in os.listdir(background_dir) if f.endswith('.jpg')]\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"Background directory not found: {background_dir}\")\n",
    "        return []\n",
    "    \n",
    "    # Filter out images that have annotations\n",
    "    valid_backgrounds = list(set(background_files) - annotated_images)\n",
    "    \n",
    "    if len(valid_backgrounds) < num_background:\n",
    "        logging.warning(\n",
    "            f\"Only {len(valid_backgrounds)} valid background images available, \"\n",
    "            f\"requested {num_background}\"\n",
    "        )\n",
    "        num_background = len(valid_backgrounds)\n",
    "    \n",
    "    # Randomly select background images\n",
    "    random.seed(random_state)\n",
    "    selected_backgrounds = random.sample(valid_backgrounds, num_background)\n",
    "    \n",
    "    logging.info(f\"Selected {len(selected_backgrounds)} background images\")\n",
    "    return selected_backgrounds\n",
    "\n",
    "def process_background_images(background_images: List[str], \n",
    "                            background_dir: str, \n",
    "                            output_dir: str):\n",
    "    \"\"\"\n",
    "    Process background images and add them to the dataset\n",
    "    Args:\n",
    "        background_images: List of background image IDs\n",
    "        background_dir: Source directory for background images\n",
    "        output_dir: Output directory for dataset\n",
    "    \"\"\"\n",
    "    for image_id in tqdm(background_images, desc=\"Processing background images\"):\n",
    "        # Create empty label file\n",
    "        label_path = os.path.join(output_dir, 'labels', f\"{image_id}.txt\")\n",
    "        open(label_path, 'w').close()  # Create empty file\n",
    "        \n",
    "        # Create symbolic link to background image\n",
    "        src_img_path = os.path.join(background_dir, f\"{image_id}.jpg\")\n",
    "        dst_img_path = os.path.join(output_dir, 'images', f\"{image_id}.jpg\")\n",
    "        \n",
    "        if os.path.exists(src_img_path):\n",
    "            if not os.path.exists(dst_img_path):\n",
    "                try:\n",
    "                    os.symlink(src_img_path, dst_img_path)\n",
    "                except OSError as e:\n",
    "                    logging.error(f\"Failed to create symlink for {image_id}: {str(e)}\")\n",
    "        else:\n",
    "            logging.warning(f\"Background image not found: {src_img_path}\")\n",
    "\n",
    "def save_results(results: List[Dict], project_dir: str):\n",
    "    \"\"\"\n",
    "    Save training results to CSV file\n",
    "    Args:\n",
    "        results: List of dictionaries containing training metrics\n",
    "        project_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    results_path = os.path.join(project_dir, 'training_metrics.csv')\n",
    "    \n",
    "    # Collect all possible column names from all results\n",
    "    csv_columns = set()\n",
    "    for result in results:\n",
    "        csv_columns.update(result.keys())\n",
    "    \n",
    "    csv_columns = sorted(list(csv_columns))  # Sort columns for consistency\n",
    "    \n",
    "    try:\n",
    "        with open(results_path, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "            writer.writeheader()\n",
    "            for result in results:\n",
    "                # Fill in missing values with None\n",
    "                row = {col: result.get(col, None) for col in csv_columns}\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        logging.info(f\"Results saved to {results_path}\")\n",
    "        \n",
    "    except IOError as e:\n",
    "        logging.error(f\"Failed to save results to CSV: {str(e)}\")\n",
    "\n",
    "def verify_dataset_integrity(dataset_dir: str, \n",
    "                           class_mapping: Dict[str, int]) -> bool:\n",
    "    \"\"\"\n",
    "    Verify the integrity of the created dataset\n",
    "    \"\"\"\n",
    "    images_dir = os.path.join(dataset_dir, 'images')\n",
    "    labels_dir = os.path.join(dataset_dir, 'labels')\n",
    "    \n",
    "    # Check directory structure\n",
    "    if not all(os.path.exists(d) for d in [images_dir, labels_dir]):\n",
    "        logging.error(\"Dataset directory structure is invalid\")\n",
    "        return False\n",
    "    \n",
    "    # Get image and label files\n",
    "    image_files = set(f[:-4] for f in os.listdir(images_dir) if f.endswith('.jpg'))\n",
    "    label_files = set(f[:-4] for f in os.listdir(labels_dir) if f.endswith('.txt'))\n",
    "    \n",
    "    # Check for missing pairs\n",
    "    missing_labels = image_files - label_files\n",
    "    missing_images = label_files - image_files\n",
    "    \n",
    "    if missing_labels:\n",
    "        logging.error(f\"Found {len(missing_labels)} images without labels\")\n",
    "        return False\n",
    "    \n",
    "    if missing_images:\n",
    "        logging.error(f\"Found {len(missing_images)} labels without images\")\n",
    "        # Clean up orphaned label files\n",
    "        for image_id in missing_images:\n",
    "            os.remove(os.path.join(labels_dir, f\"{image_id}.txt\"))\n",
    "        logging.info(\"Removed orphaned label files\")\n",
    "        \n",
    "    # Verify label format and class indices\n",
    "    valid_classes = set(class_mapping.values())\n",
    "    boxes_per_class = {idx: 0 for idx in valid_classes}\n",
    "    \n",
    "    for label_file in os.listdir(labels_dir):\n",
    "        if not label_file.endswith('.txt'):\n",
    "            continue\n",
    "            \n",
    "        with open(os.path.join(labels_dir, label_file), 'r') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    values = line.strip().split()\n",
    "                    if len(values) != 5:\n",
    "                        logging.error(f\"Invalid label format in {label_file} line {line_num}\")\n",
    "                        return False\n",
    "                    \n",
    "                    class_idx = int(values[0])\n",
    "                    if class_idx not in valid_classes:\n",
    "                        logging.error(f\"Invalid class index in {label_file} line {line_num}: {class_idx}\")\n",
    "                        return False\n",
    "                    \n",
    "                    boxes_per_class[class_idx] += 1\n",
    "                        \n",
    "                    # Verify bounding box coordinates are in range [0, 1]\n",
    "                    coords = [float(v) for v in values[1:]]\n",
    "                    if not all(0 <= v <= 1 for v in coords):\n",
    "                        logging.error(f\"Invalid coordinates in {label_file} line {line_num}\")\n",
    "                        return False\n",
    "                        \n",
    "                except ValueError as e:\n",
    "                    logging.error(f\"Invalid value in {label_file} line {line_num}: {str(e)}\")\n",
    "                    return False\n",
    "    \n",
    "    # Log box distribution\n",
    "    logging.info(\"\\nBounding box distribution:\")\n",
    "    for class_idx, count in boxes_per_class.items():\n",
    "        logging.info(f\"Class {class_idx}: {count} boxes\")\n",
    "    \n",
    "    logging.info(\"Dataset integrity verification passed\")\n",
    "    return True\n",
    "\n",
    "def verify_dataset_distribution(dataset_dir: str, \n",
    "                              class_mapping: Dict[str, int],\n",
    "                              target_images_per_class: int) -> bool:\n",
    "    \"\"\"\n",
    "    Verify the distribution of images and boxes across classes\n",
    "    \"\"\"\n",
    "    images_dir = os.path.join(dataset_dir, 'images')\n",
    "    labels_dir = os.path.join(dataset_dir, 'labels')\n",
    "    \n",
    "    # Count images and boxes per class\n",
    "    images_per_class = {idx: set() for idx in class_mapping.values()}\n",
    "    boxes_per_class = {idx: 0 for idx in class_mapping.values()}\n",
    "    \n",
    "    for label_file in os.listdir(labels_dir):\n",
    "        if not label_file.endswith('.txt'):\n",
    "            continue\n",
    "            \n",
    "        image_id = label_file[:-4]\n",
    "        class_indices_in_image = set()\n",
    "        \n",
    "        with open(os.path.join(labels_dir, label_file), 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    class_idx = int(line.strip().split()[0])\n",
    "                    boxes_per_class[class_idx] += 1\n",
    "                    class_indices_in_image.add(class_idx)\n",
    "                except (ValueError, IndexError) as e:\n",
    "                    logging.error(f\"Error parsing label file {label_file}: {str(e)}\")\n",
    "                    return False\n",
    "        \n",
    "        # Add image ID to all classes present in the image\n",
    "        for class_idx in class_indices_in_image:\n",
    "            images_per_class[class_idx].add(image_id)\n",
    "    \n",
    "    # Check class distribution\n",
    "    logging.info(\"\\nClass Distribution Analysis:\")\n",
    "    for class_idx, images in images_per_class.items():\n",
    "        image_count = len(images)\n",
    "        box_count = boxes_per_class[class_idx]\n",
    "        avg_boxes_per_image = box_count / image_count if image_count > 0 else 0\n",
    "        \n",
    "        logging.info(f\"Class {class_idx}:\")\n",
    "        logging.info(f\"  Images: {image_count} (Target: {target_images_per_class})\")\n",
    "        logging.info(f\"  Total boxes: {box_count}\")\n",
    "        logging.info(f\"  Average boxes per image: {avg_boxes_per_image:.2f}\")\n",
    "        \n",
    "        if image_count < target_images_per_class * 0.9:  # Allow 10% tolerance\n",
    "            logging.warning(f\"Class {class_idx} has fewer images than target\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# CELL 2: Data Loading and Preprocessing\n",
    "class MultiClassDataLoader:\n",
    "    def __init__(self, class_desc_path: str, annotations_path: str, image_dir: str):\n",
    "        self.class_descriptions = pd.read_csv(class_desc_path)\n",
    "        self.train_annotations = pd.read_csv(annotations_path)\n",
    "        self.image_dir = image_dir\n",
    "        self.class_mapping = {}  # LabelName to index mapping\n",
    "        self.reverse_mapping = {}  # index to DisplayName mapping\n",
    "        \n",
    "        # Get available image IDs from directory\n",
    "        self.available_image_ids = set(\n",
    "            f[:-4] for f in os.listdir(image_dir) \n",
    "            if f.endswith('.jpg')\n",
    "        )\n",
    "        logging.info(f\"Found {len(self.available_image_ids)} images in directory\")\n",
    "        \n",
    "        # Filter annotations to only include available images\n",
    "        self.train_annotations = self.train_annotations[\n",
    "            self.train_annotations['ImageID'].isin(self.available_image_ids)\n",
    "        ]\n",
    "        logging.info(f\"Filtered annotations to {len(self.train_annotations)} rows with available images\")\n",
    "        \n",
    "    def get_training_classes(self) -> List[str]:\n",
    "        \"\"\"Get list of classes marked for use in model\"\"\"\n",
    "        # Get classes marked for training\n",
    "        training_classes = self.class_descriptions[\n",
    "            self.class_descriptions['UseInModel'] == True\n",
    "        ]['DisplayName'].tolist()\n",
    "        \n",
    "        # Filter to only include classes that have sufficient data\n",
    "        valid_classes = []\n",
    "        for class_name in training_classes:\n",
    "            label_name = self.class_descriptions[\n",
    "                self.class_descriptions['DisplayName'] == class_name\n",
    "            ]['LabelName'].iloc[0]\n",
    "            \n",
    "            # Count annotations for this class\n",
    "            class_count = len(self.train_annotations[\n",
    "                self.train_annotations['LabelName'] == label_name\n",
    "            ])\n",
    "            \n",
    "            if class_count > 0:\n",
    "                valid_classes.append(class_name)\n",
    "                logging.info(f\"Class {class_name}: {class_count} annotations in available images\")\n",
    "            else:\n",
    "                logging.warning(f\"Class {class_name}: no annotations in available images, skipping\")\n",
    "        \n",
    "        logging.info(f\"Found {len(valid_classes)} classes with data in available images\")\n",
    "        return valid_classes\n",
    "    \n",
    "    def create_class_mappings(self, training_classes: List[str]):\n",
    "        \"\"\"Create mappings between class names and indices\"\"\"\n",
    "        self.class_mapping.clear()\n",
    "        self.reverse_mapping.clear()\n",
    "        \n",
    "        for idx, class_name in enumerate(training_classes):\n",
    "            label_name = self.class_descriptions[\n",
    "                self.class_descriptions['DisplayName'] == class_name\n",
    "            ]['LabelName'].iloc[0]\n",
    "            self.class_mapping[label_name] = idx\n",
    "            self.reverse_mapping[idx] = class_name\n",
    "            \n",
    "def filter_annotations(class_loader: MultiClassDataLoader,\n",
    "                      images_per_class: int,\n",
    "                      fixed_val_images: int) -> Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Filter annotations ensuring proper image counts per class\n",
    "    Args:\n",
    "        class_loader: MultiClassDataLoader instance\n",
    "        images_per_class: Number of training images per class\n",
    "        fixed_val_images: Number of validation images per class\n",
    "    Returns:\n",
    "        Tuple of (train_data, val_data) dictionaries\n",
    "    \"\"\"\n",
    "    train_data = {}\n",
    "    val_data = {}\n",
    "    \n",
    "    # Keep track of selected images across all classes\n",
    "    all_selected_train_images = set()\n",
    "    all_selected_val_images = set()\n",
    "    \n",
    "    for label_name, class_idx in class_loader.class_mapping.items():\n",
    "        # Get all annotations for this class\n",
    "        class_annotations = class_loader.train_annotations[\n",
    "            class_loader.train_annotations['LabelName'] == label_name\n",
    "        ]\n",
    "        \n",
    "        # Get unique image IDs for this class\n",
    "        class_image_ids = class_annotations['ImageID'].unique()\n",
    "        total_images_needed = images_per_class + fixed_val_images\n",
    "        \n",
    "        if len(class_image_ids) < total_images_needed:\n",
    "            logging.warning(\n",
    "                f\"Insufficient images for class {class_loader.reverse_mapping[class_idx]}. \"\n",
    "                f\"Required: {total_images_needed}, Available: {len(class_image_ids)}\"\n",
    "            )\n",
    "            continue\n",
    "        \n",
    "        # Shuffle image IDs\n",
    "        shuffled_image_ids = np.random.permutation(class_image_ids)\n",
    "        \n",
    "        # Select validation images\n",
    "        val_images = set()\n",
    "        val_idx = 0\n",
    "        while len(val_images) < fixed_val_images and val_idx < len(shuffled_image_ids):\n",
    "            img_id = shuffled_image_ids[val_idx]\n",
    "            if img_id not in all_selected_val_images:  # Avoid validation/train overlap\n",
    "                val_images.add(img_id)\n",
    "                all_selected_val_images.add(img_id)\n",
    "            val_idx += 1\n",
    "        \n",
    "        if len(val_images) < fixed_val_images:\n",
    "            logging.warning(\n",
    "                f\"Could only find {len(val_images)} unique validation images for class \"\n",
    "                f\"{class_loader.reverse_mapping[class_idx]}\"\n",
    "            )\n",
    "        \n",
    "        # Select training images\n",
    "        train_images = set()\n",
    "        train_idx = val_idx\n",
    "        while len(train_images) < images_per_class and train_idx < len(shuffled_image_ids):\n",
    "            img_id = shuffled_image_ids[train_idx]\n",
    "            if img_id not in val_images:  # Ensure no overlap with validation\n",
    "                train_images.add(img_id)\n",
    "                all_selected_train_images.add(img_id)\n",
    "            train_idx += 1\n",
    "        \n",
    "        if len(train_images) < images_per_class:\n",
    "            logging.warning(\n",
    "                f\"Could only find {len(train_images)} unique training images for class \"\n",
    "                f\"{class_loader.reverse_mapping[class_idx]}\"\n",
    "            )\n",
    "        \n",
    "        # Get all annotations for selected images\n",
    "        train_data[label_name] = class_annotations[\n",
    "            class_annotations['ImageID'].isin(train_images)\n",
    "        ]\n",
    "        val_data[label_name] = class_annotations[\n",
    "            class_annotations['ImageID'].isin(val_images)\n",
    "        ]\n",
    "        \n",
    "        # Log statistics\n",
    "        train_box_count = len(train_data[label_name])\n",
    "        val_box_count = len(val_data[label_name])\n",
    "        \n",
    "        logging.info(f\"Class {class_loader.reverse_mapping[class_idx]}:\")\n",
    "        logging.info(f\"  Training: {len(train_images)} images with {train_box_count} boxes\")\n",
    "        logging.info(f\"  Validation: {len(val_images)} images with {val_box_count} boxes\")\n",
    "    \n",
    "    # Log overall image usage statistics\n",
    "    logging.info(\"\\nOverall Dataset Statistics:\")\n",
    "    logging.info(f\"Total unique training images: {len(all_selected_train_images)}\")\n",
    "    logging.info(f\"Total unique validation images: {len(all_selected_val_images)}\")\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# CELL 3: Dataset Creation\n",
    "class MultiClassYOLODataset(Dataset):\n",
    "    def __init__(self, annotations_dict: Dict[str, pd.DataFrame], \n",
    "                 image_dir: str, class_mapping: Dict[str, int],\n",
    "                 transforms=None):\n",
    "        self.annotations_dict = annotations_dict\n",
    "        self.image_dir = image_dir\n",
    "        self.class_mapping = class_mapping\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Combine all annotations and get unique image IDs\n",
    "        all_annotations = pd.concat(annotations_dict.values())\n",
    "        self.image_ids = all_annotations['ImageID'].unique()\n",
    "        self.image_annotations = all_annotations.groupby('ImageID')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{image_id}.jpg\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Get all annotations for this image\n",
    "        image_annotations = self.image_annotations.get_group(image_id)\n",
    "        \n",
    "        # Convert bounding boxes to YOLO format\n",
    "        labels = []\n",
    "        for _, row in image_annotations.iterrows():\n",
    "            class_idx = self.class_mapping[row['LabelName']]\n",
    "            x_min, x_max = row['XMin'], row['XMax']\n",
    "            y_min, y_max = row['YMin'], row['YMax']\n",
    "            \n",
    "            x_center = (x_min + x_max) / 2\n",
    "            y_center = (y_min + y_max) / 2\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "            \n",
    "            labels.append([class_idx, x_center, y_center, width, height])\n",
    "            \n",
    "        return img_path, torch.tensor(labels)\n",
    "\n",
    "def create_multi_class_dataset(annotations_dict: Dict[str, pd.DataFrame],\n",
    "                             image_dir: str,\n",
    "                             output_dir: str,\n",
    "                             class_mapping: Dict[str, int]):\n",
    "    \"\"\"\n",
    "    Create YOLO dataset structure for multiple classes, handling multiple bounding boxes per image\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'labels'), exist_ok=True)\n",
    "    \n",
    "    # Combine all annotations\n",
    "    all_annotations = pd.concat(annotations_dict.values())\n",
    "    \n",
    "    # Get unique image IDs\n",
    "    unique_image_ids = all_annotations['ImageID'].unique()\n",
    "    logging.info(f\"Processing {len(unique_image_ids)} unique images\")\n",
    "    \n",
    "    # Process each unique image\n",
    "    for image_id in tqdm(unique_image_ids, desc=\"Creating dataset\"):\n",
    "        # Get all annotations for this image\n",
    "        image_annots = all_annotations[all_annotations['ImageID'] == image_id]\n",
    "        \n",
    "        # Create label file with all bounding boxes\n",
    "        label_path = os.path.join(output_dir, 'labels', f\"{image_id}.txt\")\n",
    "        \n",
    "        with open(label_path, 'w') as f:\n",
    "            for _, row in image_annots.iterrows():\n",
    "                class_idx = class_mapping[row['LabelName']]\n",
    "                x_center = (row['XMin'] + row['XMax']) / 2\n",
    "                y_center = (row['YMin'] + row['YMax']) / 2\n",
    "                width = row['XMax'] - row['XMin']\n",
    "                height = row['YMax'] - row['YMin']\n",
    "                \n",
    "                f.write(f\"{class_idx} {x_center} {y_center} {width} {height}\\n\")\n",
    "        \n",
    "        # Create symbolic link to image\n",
    "        src_img_path = os.path.join(image_dir, f\"{image_id}.jpg\")\n",
    "        dst_img_path = os.path.join(output_dir, 'images', f\"{image_id}.jpg\")\n",
    "        if os.path.exists(src_img_path):\n",
    "            if not os.path.exists(dst_img_path):\n",
    "                try:\n",
    "                    os.symlink(src_img_path, dst_img_path)\n",
    "                except OSError as e:\n",
    "                    logging.error(f\"Failed to create symlink for {image_id}: {str(e)}\")\n",
    "                    # If symlink fails, try copying the file\n",
    "                    try:\n",
    "                        shutil.copy2(src_img_path, dst_img_path)\n",
    "                    except IOError as e:\n",
    "                        logging.error(f\"Failed to copy image {image_id}: {str(e)}\")\n",
    "        else:\n",
    "            logging.warning(f\"Image not found: {src_img_path}\")\n",
    "            # Remove the label file if image doesn't exist\n",
    "            os.remove(label_path)\n",
    "\n",
    "\n",
    "# CELL 4: Training Functions\n",
    "def train_model(model, yaml_path, run_dir, box_count_per_class, max_epochs):\n",
    "    \"\"\"\n",
    "    Train the YOLO model with all hyperparameters\n",
    "    Args:\n",
    "        model: Initialized YOLO model\n",
    "        yaml_path: Path to dataset configuration\n",
    "        run_dir: Directory for saving results\n",
    "        box_count_per_class: Number of boxes per class used in training\n",
    "        max_epochs: Maximum number of training epochs\n",
    "    Returns:\n",
    "        Training results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = model.train(\n",
    "            data=yaml_path,\n",
    "            epochs=max_epochs,\n",
    "            imgsz=640,\n",
    "            # Model hyperparameters\n",
    "            batch=16,              # Batch size\n",
    "            workers=6,             # Number of worker threads\n",
    "            device=0,              # GPU device number\n",
    "            # conf=0.1,           # Confidence threshold\n",
    "            # iou=0.1,            # IoU threshold\n",
    "            \n",
    "            # Training parameters\n",
    "            project=run_dir,\n",
    "            name=f'yolo_boxes_{box_count_per_class}',\n",
    "            patience=20,           # Early stopping patience\n",
    "            save=True,            # Save checkpoints\n",
    "            save_period=10,       # Save every N epochs\n",
    "            \n",
    "            # Learning rate parameters\n",
    "            # lr0=0.00001,        # Initial learning rate\n",
    "            # lrf=0.000001,       # Final learning rate\n",
    "            # warmup_epochs=3,    # Number of warmup epochs\n",
    "            # warmup_momentum=0.8,# Warmup momentum\n",
    "            # warmup_bias_lr=0.1, # Warmup bias learning rate\n",
    "            \n",
    "            # Loss weights\n",
    "            # box=7.5,            # Box loss weight\n",
    "            # cls=0.5,            # Class loss weight\n",
    "            # dfl=1.5,            # DFL loss weight\n",
    "            \n",
    "            # Visualization and logging\n",
    "            plots=True,           # Generate plots\n",
    "            verbose=True,         # Verbose output\n",
    "            \n",
    "            # Regularization\n",
    "            # weight_decay=0.0005,# Weight decay\n",
    "            # dropout=0.2,        # Dropout rate\n",
    "            \n",
    "            # Data augmentation (enabled by default)\n",
    "            # hsv_h=0,           # HSV-Hue augmentation\n",
    "            # hsv_s=0,           # HSV-Saturation augmentation\n",
    "            # hsv_v=0,           # HSV-Value augmentation\n",
    "            # translate=0,        # Translation augmentation\n",
    "            # scale=0,           # Scaling augmentation\n",
    "            # fliplr=0,          # Horizontal flip augmentation\n",
    "            # mosaic=0,          # Mosaic augmentation\n",
    "            # erasing=0,         # Random erasing\n",
    "            # crop_fraction=1     # Crop fraction\n",
    "        )\n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed with error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def save_training_results(results, run_dir, model_dir, box_count_per_class):\n",
    "    \"\"\"Save model checkpoints and training metrics\"\"\"\n",
    "    try:\n",
    "        # Save models\n",
    "        yolo_output_dir = os.path.join(run_dir, f'yolo_boxes_{box_count_per_class}')\n",
    "        \n",
    "        # Save best model\n",
    "        if os.path.exists(os.path.join(yolo_output_dir, 'weights', 'best.pt')):\n",
    "            best_model_path = os.path.join(model_dir, f'model_best_boxes_{box_count_per_class}.pt')\n",
    "            shutil.copy2(\n",
    "                os.path.join(yolo_output_dir, 'weights', 'best.pt'),\n",
    "                best_model_path\n",
    "            )\n",
    "        \n",
    "        # Save final model\n",
    "        if os.path.exists(os.path.join(yolo_output_dir, 'weights', 'last.pt')):\n",
    "            final_model_path = os.path.join(model_dir, f'model_final_boxes_{box_count_per_class}.pt')\n",
    "            shutil.copy2(\n",
    "                os.path.join(yolo_output_dir, 'weights', 'last.pt'),\n",
    "                final_model_path\n",
    "            )\n",
    "        \n",
    "        # Collect metrics\n",
    "        metrics = {\n",
    "            'BoundingBoxCountPerClass': box_count_per_class,\n",
    "            'ModelDirectory': model_dir,\n",
    "            'FinalEpoch': results.epoch\n",
    "        }\n",
    "        \n",
    "        # Add per-class metrics if available\n",
    "        if hasattr(results, 'results_dict'):\n",
    "            metrics_dict = results.results_dict\n",
    "            \n",
    "            # Overall metrics\n",
    "            metrics.update({\n",
    "                'Precision': float(metrics_dict.get('metrics/precision', 0.0)),\n",
    "                'Recall': float(metrics_dict.get('metrics/recall', 0.0)),\n",
    "                'mAP50': float(metrics_dict.get('metrics/mAP50', 0.0)),\n",
    "                'mAP50-95': float(metrics_dict.get('metrics/mAP50-95', 0.0))\n",
    "            })\n",
    "            \n",
    "            # Add per-class metrics if available\n",
    "            for class_idx in range(results.num_classes):\n",
    "                class_prefix = f'metrics/precision_{class_idx}'\n",
    "                if class_prefix in metrics_dict:\n",
    "                    metrics.update({\n",
    "                        f'Precision_Class_{class_idx}': float(metrics_dict[f'metrics/precision_{class_idx}']),\n",
    "                        f'Recall_Class_{class_idx}': float(metrics_dict[f'metrics/recall_{class_idx}']),\n",
    "                        f'mAP50_Class_{class_idx}': float(metrics_dict[f'metrics/mAP50_{class_idx}']),\n",
    "                        f'mAP50-95_Class_{class_idx}': float(metrics_dict[f'metrics/mAP50-95_{class_idx}'])\n",
    "                    })\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save training results: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def train_multi_class_yolo(image_count_per_class: int,\n",
    "                          image_dir: str,\n",
    "                          project_dir: str,\n",
    "                          class_loader: MultiClassDataLoader,\n",
    "                          fixed_val_size: int = 500,\n",
    "                          max_epochs: int = 300,\n",
    "                          background_percentage: float = 0):  # Set default to 0\n",
    "    \"\"\"Train YOLO model for multiple classes\"\"\"\n",
    "    \n",
    "    # Create run directory\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(project_dir, f'multi_class_run_{image_count_per_class}_images_{timestamp}')\n",
    "    dataset_dir = os.path.join(run_dir, 'dataset')\n",
    "    model_dir = os.path.join(run_dir, 'models')\n",
    "    \n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Get training classes and create mappings\n",
    "    training_classes = class_loader.get_training_classes()\n",
    "    class_loader.create_class_mappings(training_classes)\n",
    "    \n",
    "    # Filter annotations for each class - now returns tuple (train_data, val_data)\n",
    "    train_data, val_data = filter_annotations(class_loader, image_count_per_class, fixed_val_size)\n",
    "    \n",
    "    if not train_data or not val_data:\n",
    "        logging.error(\"No classes had sufficient data for training\")\n",
    "        return None\n",
    "    \n",
    "    # Create dataset structure\n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    val_dir = os.path.join(dataset_dir, 'val')\n",
    "    \n",
    "    # Create training dataset\n",
    "    create_multi_class_dataset(\n",
    "        train_data, \n",
    "        image_dir, \n",
    "        train_dir,\n",
    "        class_loader.class_mapping\n",
    "    )\n",
    "    \n",
    "    # Verify training dataset integrity\n",
    "    logging.info(\"Verifying training dataset integrity...\")\n",
    "    if not verify_dataset_integrity(train_dir, class_loader.class_mapping):\n",
    "        logging.error(\"Training dataset verification failed. Aborting training.\")\n",
    "        return None\n",
    "    \n",
    "    # Create validation dataset\n",
    "    create_multi_class_dataset(\n",
    "        val_data, \n",
    "        image_dir, \n",
    "        val_dir,\n",
    "        class_loader.class_mapping\n",
    "    )\n",
    "    \n",
    "    # Verify validation dataset integrity\n",
    "    logging.info(\"Verifying validation dataset integrity...\")\n",
    "    if not verify_dataset_integrity(val_dir, class_loader.class_mapping):\n",
    "        logging.error(\"Validation dataset verification failed. Aborting training.\")\n",
    "        return None\n",
    "    \n",
    "    # Create YAML configuration\n",
    "    yaml_path = os.path.join(run_dir, 'dataset.yaml')\n",
    "    create_yaml_file(\n",
    "        train_dir, val_dir, yaml_path,\n",
    "        num_classes=len(training_classes),\n",
    "        class_names=[class_loader.reverse_mapping[i] for i in range(len(training_classes))]\n",
    "    )\n",
    "    \n",
    "    # Print dataset summary\n",
    "    logging.info(\"\\nDataset Summary:\")\n",
    "    logging.info(f\"Number of classes: {len(training_classes)}\")\n",
    "    \n",
    "    # Calculate total images and boxes\n",
    "    train_images = len(os.listdir(os.path.join(train_dir, 'images')))\n",
    "    val_images = len(os.listdir(os.path.join(val_dir, 'images')))\n",
    "    \n",
    "    logging.info(f\"Training images: {train_images}\")\n",
    "    logging.info(f\"Validation images: {val_images}\")\n",
    "    \n",
    "    # Class distribution summary\n",
    "    logging.info(\"\\nClass Distribution:\")\n",
    "    for class_idx, class_name in class_loader.reverse_mapping.items():\n",
    "        # Count boxes in training set\n",
    "        train_count = sum(1 for f in os.listdir(os.path.join(train_dir, 'labels'))\n",
    "                         for line in open(os.path.join(train_dir, 'labels', f))\n",
    "                         if line.startswith(f\"{class_idx} \"))\n",
    "        \n",
    "        # Count boxes in validation set\n",
    "        val_count = sum(1 for f in os.listdir(os.path.join(val_dir, 'labels'))\n",
    "                       for line in open(os.path.join(val_dir, 'labels', f))\n",
    "                       if line.startswith(f\"{class_idx} \"))\n",
    "        \n",
    "        logging.info(f\"Class {class_name}: {train_count} training boxes, {val_count} validation boxes\")\n",
    "    \n",
    "    # Initialize and train model\n",
    "    try:\n",
    "        model = YOLO('yolov8s.pt')\n",
    "        results = train_model(model, yaml_path, run_dir, image_count_per_class, max_epochs)\n",
    "        save_training_results(results, run_dir, model_dir, image_count_per_class)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during training: {str(e)}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 09:53:08,903 - INFO - Found 102217 images in directory\n",
      "2024-11-24 09:53:10,058 - INFO - Filtered annotations to 1061477 rows with available images\n",
      "2024-11-24 09:53:10,132 - INFO - Class Footwear: 75466 annotations in available images\n",
      "2024-11-24 09:53:10,188 - INFO - Class Suit: 15390 annotations in available images\n",
      "2024-11-24 09:53:10,246 - INFO - Class Glasses: 9346 annotations in available images\n",
      "2024-11-24 09:53:10,300 - INFO - Class Dress: 9040 annotations in available images\n",
      "2024-11-24 09:53:10,360 - INFO - Class Jeans: 15804 annotations in available images\n",
      "2024-11-24 09:53:10,420 - INFO - Class Tire: 22192 annotations in available images\n",
      "2024-11-24 09:53:10,509 - INFO - Class Fashion accessory: 19463 annotations in available images\n",
      "2024-11-24 09:53:10,598 - INFO - Class Microphone: 7432 annotations in available images\n",
      "2024-11-24 09:53:10,658 - INFO - Class Guitar: 6723 annotations in available images\n",
      "2024-11-24 09:53:10,716 - INFO - Class Toy: 19766 annotations in available images\n",
      "2024-11-24 09:53:10,769 - INFO - Class Poster: 6966 annotations in available images\n",
      "2024-11-24 09:53:10,826 - INFO - Class Drink: 11481 annotations in available images\n",
      "2024-11-24 09:53:10,883 - INFO - Class Bicycle wheel: 17685 annotations in available images\n",
      "2024-11-24 09:53:10,940 - INFO - Class Furniture: 11833 annotations in available images\n",
      "2024-11-24 09:53:10,996 - INFO - Class Door: 6462 annotations in available images\n",
      "2024-11-24 09:53:11,051 - INFO - Class Jacket: 9365 annotations in available images\n",
      "2024-11-24 09:53:11,103 - INFO - Class Baked goods: 10075 annotations in available images\n",
      "2024-11-24 09:53:11,156 - INFO - Class Desk: 5663 annotations in available images\n",
      "2024-11-24 09:53:11,210 - INFO - Class Hat: 7109 annotations in available images\n",
      "2024-11-24 09:53:11,269 - INFO - Class Flag: 15650 annotations in available images\n",
      "2024-11-24 09:53:11,325 - INFO - Class Shorts: 11196 annotations in available images\n",
      "2024-11-24 09:53:11,381 - INFO - Class Musical instrument: 14185 annotations in available images\n",
      "2024-11-24 09:53:11,433 - INFO - Class Shirt: 6449 annotations in available images\n",
      "2024-11-24 09:53:11,490 - INFO - Class Camera: 5702 annotations in available images\n",
      "2024-11-24 09:53:11,547 - INFO - Class Beer: 9294 annotations in available images\n",
      "2024-11-24 09:53:11,548 - INFO - Found 25 classes with data in available images\n",
      "2024-11-24 09:53:11,651 - INFO - Class Footwear:\n",
      "2024-11-24 09:53:11,652 - INFO -   Training: 500 images with 2676 boxes\n",
      "2024-11-24 09:53:11,653 - INFO -   Validation: 500 images with 2758 boxes\n",
      "2024-11-24 09:53:11,720 - INFO - Class Suit:\n",
      "2024-11-24 09:53:11,727 - INFO -   Training: 500 images with 1116 boxes\n",
      "2024-11-24 09:53:11,728 - INFO -   Validation: 500 images with 1096 boxes\n",
      "2024-11-24 09:53:11,790 - INFO - Class Glasses:\n",
      "2024-11-24 09:53:11,792 - INFO -   Training: 500 images with 682 boxes\n",
      "2024-11-24 09:53:11,793 - INFO -   Validation: 500 images with 678 boxes\n",
      "2024-11-24 09:53:11,852 - INFO - Class Dress:\n",
      "2024-11-24 09:53:11,854 - INFO -   Training: 500 images with 850 boxes\n",
      "2024-11-24 09:53:11,855 - INFO -   Validation: 500 images with 814 boxes\n",
      "2024-11-24 09:53:11,918 - INFO - Class Jeans:\n",
      "2024-11-24 09:53:11,919 - INFO -   Training: 500 images with 1225 boxes\n",
      "2024-11-24 09:53:11,920 - INFO -   Validation: 500 images with 1190 boxes\n",
      "2024-11-24 09:53:11,989 - INFO - Class Tire:\n",
      "2024-11-24 09:53:11,991 - INFO -   Training: 500 images with 2003 boxes\n",
      "2024-11-24 09:53:11,991 - INFO -   Validation: 500 images with 2065 boxes\n",
      "2024-11-24 09:53:12,058 - INFO - Class Fashion accessory:\n",
      "2024-11-24 09:53:12,060 - INFO -   Training: 500 images with 1960 boxes\n",
      "2024-11-24 09:53:12,061 - INFO -   Validation: 500 images with 1989 boxes\n",
      "2024-11-24 09:53:12,130 - INFO - Class Microphone:\n",
      "2024-11-24 09:53:12,131 - INFO -   Training: 500 images with 683 boxes\n",
      "2024-11-24 09:53:12,132 - INFO -   Validation: 500 images with 671 boxes\n",
      "2024-11-24 09:53:12,192 - INFO - Class Guitar:\n",
      "2024-11-24 09:53:12,194 - INFO -   Training: 500 images with 746 boxes\n",
      "2024-11-24 09:53:12,195 - INFO -   Validation: 500 images with 745 boxes\n",
      "2024-11-24 09:53:12,265 - INFO - Class Toy:\n",
      "2024-11-24 09:53:12,267 - INFO -   Training: 500 images with 2331 boxes\n",
      "2024-11-24 09:53:12,267 - INFO -   Validation: 500 images with 1803 boxes\n",
      "2024-11-24 09:53:12,325 - INFO - Class Poster:\n",
      "2024-11-24 09:53:12,326 - INFO -   Training: 500 images with 782 boxes\n",
      "2024-11-24 09:53:12,327 - INFO -   Validation: 500 images with 762 boxes\n",
      "2024-11-24 09:53:12,389 - INFO - Class Drink:\n",
      "2024-11-24 09:53:12,390 - INFO -   Training: 500 images with 1193 boxes\n",
      "2024-11-24 09:53:12,391 - INFO -   Validation: 500 images with 1212 boxes\n",
      "2024-11-24 09:53:12,454 - INFO - Class Bicycle wheel:\n",
      "2024-11-24 09:53:12,456 - INFO -   Training: 500 images with 1885 boxes\n",
      "2024-11-24 09:53:12,456 - INFO -   Validation: 500 images with 2200 boxes\n",
      "2024-11-24 09:53:12,519 - INFO - Class Furniture:\n",
      "2024-11-24 09:53:12,521 - INFO -   Training: 500 images with 1278 boxes\n",
      "2024-11-24 09:53:12,522 - INFO -   Validation: 500 images with 1328 boxes\n",
      "2024-11-24 09:53:12,582 - INFO - Class Door:\n",
      "2024-11-24 09:53:12,583 - INFO -   Training: 500 images with 719 boxes\n",
      "2024-11-24 09:53:12,584 - INFO -   Validation: 500 images with 735 boxes\n",
      "2024-11-24 09:53:12,642 - INFO - Class Jacket:\n",
      "2024-11-24 09:53:12,643 - INFO -   Training: 500 images with 1045 boxes\n",
      "2024-11-24 09:53:12,644 - INFO -   Validation: 500 images with 1018 boxes\n",
      "2024-11-24 09:53:12,700 - INFO - Class Baked goods:\n",
      "2024-11-24 09:53:12,701 - INFO -   Training: 500 images with 946 boxes\n",
      "2024-11-24 09:53:12,702 - INFO -   Validation: 500 images with 1196 boxes\n",
      "2024-11-24 09:53:12,759 - INFO - Class Desk:\n",
      "2024-11-24 09:53:12,760 - INFO -   Training: 500 images with 598 boxes\n",
      "2024-11-24 09:53:12,761 - INFO -   Validation: 500 images with 611 boxes\n",
      "2024-11-24 09:53:12,817 - INFO - Class Hat:\n",
      "2024-11-24 09:53:12,818 - INFO -   Training: 500 images with 733 boxes\n",
      "2024-11-24 09:53:12,819 - INFO -   Validation: 500 images with 765 boxes\n",
      "2024-11-24 09:53:12,882 - INFO - Class Flag:\n",
      "2024-11-24 09:53:12,883 - INFO -   Training: 500 images with 1740 boxes\n",
      "2024-11-24 09:53:12,883 - INFO -   Validation: 500 images with 1639 boxes\n",
      "2024-11-24 09:53:12,943 - INFO - Class Shorts:\n",
      "2024-11-24 09:53:12,944 - INFO -   Training: 500 images with 1220 boxes\n",
      "2024-11-24 09:53:12,945 - INFO -   Validation: 500 images with 1258 boxes\n",
      "2024-11-24 09:53:13,006 - INFO - Class Musical instrument:\n",
      "2024-11-24 09:53:13,008 - INFO -   Training: 500 images with 1573 boxes\n",
      "2024-11-24 09:53:13,008 - INFO -   Validation: 500 images with 1559 boxes\n",
      "2024-11-24 09:53:13,066 - INFO - Class Shirt:\n",
      "2024-11-24 09:53:13,067 - INFO -   Training: 500 images with 717 boxes\n",
      "2024-11-24 09:53:13,068 - INFO -   Validation: 500 images with 701 boxes\n",
      "2024-11-24 09:53:13,132 - INFO - Class Camera:\n",
      "2024-11-24 09:53:13,133 - INFO -   Training: 500 images with 642 boxes\n",
      "2024-11-24 09:53:13,134 - INFO -   Validation: 500 images with 640 boxes\n",
      "2024-11-24 09:53:13,197 - INFO - Class Beer:\n",
      "2024-11-24 09:53:13,199 - INFO -   Training: 500 images with 999 boxes\n",
      "2024-11-24 09:53:13,200 - INFO -   Validation: 500 images with 996 boxes\n",
      "2024-11-24 09:53:13,201 - INFO - \n",
      "Overall Dataset Statistics:\n",
      "2024-11-24 09:53:13,201 - INFO - Total unique training images: 12282\n",
      "2024-11-24 09:53:13,202 - INFO - Total unique validation images: 12500\n",
      "2024-11-24 09:53:13,217 - INFO - Processing 12282 unique images\n",
      "2024-11-24 09:54:03,008 - INFO - Verifying training dataset integrity...\n",
      "2024-11-24 09:54:04,574 - INFO - \n",
      "Bounding box distribution:\n",
      "2024-11-24 09:54:04,576 - INFO - Class 0: 2676 boxes\n",
      "2024-11-24 09:54:04,576 - INFO - Class 1: 1116 boxes\n",
      "2024-11-24 09:54:04,577 - INFO - Class 2: 682 boxes\n",
      "2024-11-24 09:54:04,578 - INFO - Class 3: 850 boxes\n",
      "2024-11-24 09:54:04,579 - INFO - Class 4: 1225 boxes\n",
      "2024-11-24 09:54:04,579 - INFO - Class 5: 2003 boxes\n",
      "2024-11-24 09:54:04,580 - INFO - Class 6: 1960 boxes\n",
      "2024-11-24 09:54:04,581 - INFO - Class 7: 683 boxes\n",
      "2024-11-24 09:54:04,581 - INFO - Class 8: 746 boxes\n",
      "2024-11-24 09:54:04,582 - INFO - Class 9: 2331 boxes\n",
      "2024-11-24 09:54:04,582 - INFO - Class 10: 782 boxes\n",
      "2024-11-24 09:54:04,583 - INFO - Class 11: 1193 boxes\n",
      "2024-11-24 09:54:04,584 - INFO - Class 12: 1885 boxes\n",
      "2024-11-24 09:54:04,584 - INFO - Class 13: 1278 boxes\n",
      "2024-11-24 09:54:04,585 - INFO - Class 14: 719 boxes\n",
      "2024-11-24 09:54:04,585 - INFO - Class 15: 1045 boxes\n",
      "2024-11-24 09:54:04,586 - INFO - Class 16: 946 boxes\n",
      "2024-11-24 09:54:04,587 - INFO - Class 17: 598 boxes\n",
      "2024-11-24 09:54:04,587 - INFO - Class 18: 733 boxes\n",
      "2024-11-24 09:54:04,588 - INFO - Class 19: 1740 boxes\n",
      "2024-11-24 09:54:04,589 - INFO - Class 20: 1220 boxes\n",
      "2024-11-24 09:54:04,589 - INFO - Class 21: 1573 boxes\n",
      "2024-11-24 09:54:04,590 - INFO - Class 22: 717 boxes\n",
      "2024-11-24 09:54:04,591 - INFO - Class 23: 642 boxes\n",
      "2024-11-24 09:54:04,591 - INFO - Class 24: 999 boxes\n",
      "2024-11-24 09:54:04,592 - INFO - Dataset integrity verification passed\n",
      "2024-11-24 09:54:04,606 - INFO - Processing 12500 unique images\n",
      "2024-11-24 09:54:55,454 - INFO - Verifying validation dataset integrity...\n",
      "2024-11-24 09:54:57,123 - INFO - \n",
      "Bounding box distribution:\n",
      "2024-11-24 09:54:57,124 - INFO - Class 0: 2758 boxes\n",
      "2024-11-24 09:54:57,124 - INFO - Class 1: 1096 boxes\n",
      "2024-11-24 09:54:57,125 - INFO - Class 2: 678 boxes\n",
      "2024-11-24 09:54:57,126 - INFO - Class 3: 814 boxes\n",
      "2024-11-24 09:54:57,126 - INFO - Class 4: 1190 boxes\n",
      "2024-11-24 09:54:57,127 - INFO - Class 5: 2065 boxes\n",
      "2024-11-24 09:54:57,128 - INFO - Class 6: 1989 boxes\n",
      "2024-11-24 09:54:57,128 - INFO - Class 7: 671 boxes\n",
      "2024-11-24 09:54:57,129 - INFO - Class 8: 745 boxes\n",
      "2024-11-24 09:54:57,129 - INFO - Class 9: 1803 boxes\n",
      "2024-11-24 09:54:57,130 - INFO - Class 10: 762 boxes\n",
      "2024-11-24 09:54:57,130 - INFO - Class 11: 1212 boxes\n",
      "2024-11-24 09:54:57,131 - INFO - Class 12: 2200 boxes\n",
      "2024-11-24 09:54:57,131 - INFO - Class 13: 1328 boxes\n",
      "2024-11-24 09:54:57,132 - INFO - Class 14: 735 boxes\n",
      "2024-11-24 09:54:57,133 - INFO - Class 15: 1018 boxes\n",
      "2024-11-24 09:54:57,134 - INFO - Class 16: 1196 boxes\n",
      "2024-11-24 09:54:57,134 - INFO - Class 17: 611 boxes\n",
      "2024-11-24 09:54:57,135 - INFO - Class 18: 765 boxes\n",
      "2024-11-24 09:54:57,135 - INFO - Class 19: 1639 boxes\n",
      "2024-11-24 09:54:57,136 - INFO - Class 20: 1258 boxes\n",
      "2024-11-24 09:54:57,136 - INFO - Class 21: 1559 boxes\n",
      "2024-11-24 09:54:57,137 - INFO - Class 22: 701 boxes\n",
      "2024-11-24 09:54:57,137 - INFO - Class 23: 640 boxes\n",
      "2024-11-24 09:54:57,138 - INFO - Class 24: 996 boxes\n",
      "2024-11-24 09:54:57,139 - INFO - Dataset integrity verification passed\n",
      "2024-11-24 09:54:57,143 - INFO - Created YAML configuration file at E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310\\dataset.yaml\n",
      "2024-11-24 09:54:57,144 - INFO - \n",
      "Dataset Summary:\n",
      "2024-11-24 09:54:57,144 - INFO - Number of classes: 25\n",
      "2024-11-24 09:54:57,163 - INFO - Training images: 12282\n",
      "2024-11-24 09:54:57,164 - INFO - Validation images: 12500\n",
      "2024-11-24 09:54:57,164 - INFO - \n",
      "Class Distribution:\n",
      "2024-11-24 09:55:00,206 - INFO - Class Footwear: 2676 training boxes, 2758 validation boxes\n",
      "2024-11-24 09:55:03,260 - INFO - Class Suit: 1116 training boxes, 1096 validation boxes\n",
      "2024-11-24 09:55:06,334 - INFO - Class Glasses: 682 training boxes, 678 validation boxes\n",
      "2024-11-24 09:55:09,324 - INFO - Class Dress: 850 training boxes, 814 validation boxes\n",
      "2024-11-24 09:55:12,366 - INFO - Class Jeans: 1225 training boxes, 1190 validation boxes\n",
      "2024-11-24 09:55:15,385 - INFO - Class Tire: 2003 training boxes, 2065 validation boxes\n",
      "2024-11-24 09:55:18,430 - INFO - Class Fashion accessory: 1960 training boxes, 1989 validation boxes\n",
      "2024-11-24 09:55:21,439 - INFO - Class Microphone: 683 training boxes, 671 validation boxes\n",
      "2024-11-24 09:55:24,459 - INFO - Class Guitar: 746 training boxes, 745 validation boxes\n",
      "2024-11-24 09:55:27,472 - INFO - Class Toy: 2331 training boxes, 1803 validation boxes\n",
      "2024-11-24 09:55:30,449 - INFO - Class Poster: 782 training boxes, 762 validation boxes\n",
      "2024-11-24 09:55:33,497 - INFO - Class Drink: 1193 training boxes, 1212 validation boxes\n",
      "2024-11-24 09:55:36,570 - INFO - Class Bicycle wheel: 1885 training boxes, 2200 validation boxes\n",
      "2024-11-24 09:55:39,514 - INFO - Class Furniture: 1278 training boxes, 1328 validation boxes\n",
      "2024-11-24 09:55:42,484 - INFO - Class Door: 719 training boxes, 735 validation boxes\n",
      "2024-11-24 09:55:45,434 - INFO - Class Jacket: 1045 training boxes, 1018 validation boxes\n",
      "2024-11-24 09:55:48,373 - INFO - Class Baked goods: 946 training boxes, 1196 validation boxes\n",
      "2024-11-24 09:55:51,319 - INFO - Class Desk: 598 training boxes, 611 validation boxes\n",
      "2024-11-24 09:55:54,328 - INFO - Class Hat: 733 training boxes, 765 validation boxes\n",
      "2024-11-24 09:55:57,328 - INFO - Class Flag: 1740 training boxes, 1639 validation boxes\n",
      "2024-11-24 09:56:00,338 - INFO - Class Shorts: 1220 training boxes, 1258 validation boxes\n",
      "2024-11-24 09:56:03,338 - INFO - Class Musical instrument: 1573 training boxes, 1559 validation boxes\n",
      "2024-11-24 09:56:06,278 - INFO - Class Shirt: 717 training boxes, 701 validation boxes\n",
      "2024-11-24 09:56:09,241 - INFO - Class Camera: 642 training boxes, 640 validation boxes\n",
      "2024-11-24 09:56:12,223 - INFO - Class Beer: 999 training boxes, 996 validation boxes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.36 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.23  Python-3.10.0 torch-2.5.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3080, 10240MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310\\dataset.yaml, epochs=300, time=None, patience=20, batch=16, imgsz=640, save=True, save_period=10, cache=False, device=0, workers=6, project=E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310, name=yolo_boxes_500, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310\\yolo_boxes_500\n",
      "Overriding model.yaml nc=80 with nc=25\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2125723  ultralytics.nn.modules.head.Detect           [25, [128, 256, 512]]         \n",
      "Model summary: 225 layers, 11,145,275 parameters, 11,145,259 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310\\yolo_boxes_500', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310\\dataset\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310\\dataset\\val\\labels.cache\n",
      "Plotting labels to E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310\\yolo_boxes_500\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 6 dataloader workers\n",
      "Logging results to \u001b[1mE:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310\\yolo_boxes_500\u001b[0m\n",
      "Starting training for 300 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.301      0.362      0.251      0.147\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.288      0.316      0.223      0.124\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.258      0.251      0.158     0.0799\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.276      0.249       0.16      0.083\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.349      0.289      0.198      0.104\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.338      0.281      0.212      0.112\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.282      0.315      0.216      0.119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.334      0.331      0.238       0.13\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.303      0.325      0.248      0.139\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.305      0.334      0.246      0.138\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.369      0.345      0.266      0.151\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.354      0.352      0.262      0.148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429       0.35      0.373      0.289      0.166\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.353      0.362      0.281      0.161\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.344      0.363      0.284      0.163\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.344        0.4        0.3      0.175\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.353      0.394      0.303      0.174\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429       0.35      0.401      0.306       0.18\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.359      0.394      0.306      0.178\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.347      0.414      0.315      0.186\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.359      0.402       0.31      0.183\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.364      0.395      0.314      0.184\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.356       0.42      0.318      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.378      0.408      0.326      0.195\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.358       0.42      0.326      0.195\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.378      0.419      0.329      0.196\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.378      0.418      0.332      0.199\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.375      0.422       0.33      0.197\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.392      0.414      0.339      0.203\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.395      0.418      0.341      0.205\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.395      0.423      0.348       0.21\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.393      0.424      0.343      0.207\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.387      0.431      0.346       0.21\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.389      0.427      0.345       0.21\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.395      0.426      0.348      0.211\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.399      0.427      0.347       0.21\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.387      0.439      0.348      0.212\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.399      0.429      0.351      0.214\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.392      0.441      0.352      0.213\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.389      0.438      0.351      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.386      0.445      0.353      0.215\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.391      0.443      0.354      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.395      0.432      0.351      0.214\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429       0.39      0.443      0.352      0.215\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.395      0.438      0.353      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.395      0.441      0.356      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.397      0.443      0.357      0.218\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.404      0.435      0.355      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.395      0.441      0.351      0.214\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.398       0.44      0.353      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.397      0.441      0.354      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.395      0.444      0.355      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.392      0.446      0.356      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.394      0.445      0.354      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.396      0.442      0.353      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.391      0.449      0.353      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.392      0.444      0.353      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.395      0.444      0.352      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.396      0.445      0.353      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.395      0.444      0.353      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.396      0.447      0.353      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.406      0.441      0.355      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.402      0.442      0.356      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.401      0.444      0.355      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.403      0.441      0.354      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.399      0.443      0.354      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30429      0.402      0.442      0.353      0.216\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 20 epochs. Best results observed at epoch 47, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=20) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "67 epochs completed in 5.926 hours.\n",
      "Optimizer stripped from E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310\\yolo_boxes_500\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310\\yolo_boxes_500\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310\\yolo_boxes_500\\weights\\best.pt...\n",
      "Ultralytics 8.3.23  Python-3.10.0 torch-2.5.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3080, 10240MiB)\n",
      "Model summary (fused): 168 layers, 11,135,259 parameters, 0 gradients, 28.5 GFLOPs\n",
      "                   all      12500      30429      0.396      0.443      0.357      0.218\n",
      "              Footwear        500       2758      0.193     0.0556     0.0908     0.0381\n",
      "                  Suit        500       1096      0.423      0.558      0.395      0.236\n",
      "               Glasses        500        678      0.387      0.445       0.35      0.173\n",
      "                 Dress        500        814        0.4       0.51      0.382      0.248\n",
      "                 Jeans        500       1190      0.297      0.352      0.225       0.13\n",
      "                  Tire        500       2065      0.449      0.412      0.381      0.223\n",
      "     Fashion accessory        500       1989      0.222     0.0558     0.0521     0.0257\n",
      "            Microphone        500        671      0.373      0.376       0.27      0.116\n",
      "                Guitar        500        745      0.577      0.678      0.575       0.36\n",
      "                   Toy        500       1803      0.385      0.318      0.277      0.154\n",
      "                Poster        500        762       0.43       0.57      0.449      0.364\n",
      "                 Drink        500       1212      0.256      0.171       0.14     0.0691\n",
      "         Bicycle wheel        500       2200      0.558      0.607      0.521      0.322\n",
      "             Furniture        500       1328      0.175      0.109     0.0832     0.0459\n",
      "                  Door        500        735      0.353      0.457      0.372      0.203\n",
      "                Jacket        500       1018      0.392      0.458      0.338      0.192\n",
      "           Baked goods        500       1196      0.559      0.537      0.527       0.37\n",
      "                  Desk        500        611      0.419      0.544      0.412      0.214\n",
      "                   Hat        500        765      0.385      0.583       0.38      0.289\n",
      "                  Flag        500       1639      0.505      0.563      0.502      0.286\n",
      "                Shorts        500       1258      0.484      0.607      0.458      0.247\n",
      "    Musical instrument        500       1559      0.247      0.108     0.0889     0.0347\n",
      "                 Shirt        500        701      0.428      0.593      0.442      0.327\n",
      "                Camera        500        640      0.595      0.756      0.729      0.474\n",
      "                  Beer        500        996      0.419      0.643      0.478      0.307\n",
      "Speed: 0.2ms preprocess, 1.6ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_500_images_20241124_095310\\yolo_boxes_500\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 15:55:30,067 - ERROR - Failed to save training results: 'DetMetrics' object has no attribute 'epoch'. See valid attributes below.\n",
      "\n",
      "    Utility class for computing detection metrics such as precision, recall, and mean average precision (mAP) of an\n",
      "    object detection model.\n",
      "\n",
      "    Args:\n",
      "        save_dir (Path): A path to the directory where the output plots will be saved. Defaults to current directory.\n",
      "        plot (bool): A flag that indicates whether to plot precision-recall curves for each class. Defaults to False.\n",
      "        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n",
      "        names (dict of str): A dict of strings that represents the names of the classes. Defaults to an empty tuple.\n",
      "\n",
      "    Attributes:\n",
      "        save_dir (Path): A path to the directory where the output plots will be saved.\n",
      "        plot (bool): A flag that indicates whether to plot the precision-recall curves for each class.\n",
      "        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n",
      "        names (dict of str): A dict of strings that represents the names of the classes.\n",
      "        box (Metric): An instance of the Metric class for storing the results of the detection metrics.\n",
      "        speed (dict): A dictionary for storing the execution time of different parts of the detection process.\n",
      "\n",
      "    Methods:\n",
      "        process(tp, conf, pred_cls, target_cls): Updates the metric results with the latest batch of predictions.\n",
      "        keys: Returns a list of keys for accessing the computed detection metrics.\n",
      "        mean_results: Returns a list of mean values for the computed detection metrics.\n",
      "        class_result(i): Returns a list of values for the computed detection metrics for a specific class.\n",
      "        maps: Returns a dictionary of mean average precision (mAP) values for different IoU thresholds.\n",
      "        fitness: Computes the fitness score based on the computed detection metrics.\n",
      "        ap_class_index: Returns a list of class indices sorted by their average precision (AP) values.\n",
      "        results_dict: Returns a dictionary that maps detection metric keys to their computed values.\n",
      "        curves: TODO\n",
      "        curves_results: TODO\n",
      "    \n",
      "2024-11-24 15:55:30,076 - ERROR - Error during training with 500 images per class: 'list' object is not callable\n",
      "2024-11-24 15:55:30,186 - INFO - Class Footwear: 75466 annotations in available images\n",
      "2024-11-24 15:55:30,256 - INFO - Class Suit: 15390 annotations in available images\n",
      "2024-11-24 15:55:30,315 - INFO - Class Glasses: 9346 annotations in available images\n",
      "2024-11-24 15:55:30,382 - INFO - Class Dress: 9040 annotations in available images\n",
      "2024-11-24 15:55:30,443 - INFO - Class Jeans: 15804 annotations in available images\n",
      "2024-11-24 15:55:30,525 - INFO - Class Tire: 22192 annotations in available images\n",
      "2024-11-24 15:55:30,584 - INFO - Class Fashion accessory: 19463 annotations in available images\n",
      "2024-11-24 15:55:30,642 - INFO - Class Microphone: 7432 annotations in available images\n",
      "2024-11-24 15:55:30,700 - INFO - Class Guitar: 6723 annotations in available images\n",
      "2024-11-24 15:55:30,760 - INFO - Class Toy: 19766 annotations in available images\n",
      "2024-11-24 15:55:30,814 - INFO - Class Poster: 6966 annotations in available images\n",
      "2024-11-24 15:55:30,874 - INFO - Class Drink: 11481 annotations in available images\n",
      "2024-11-24 15:55:30,933 - INFO - Class Bicycle wheel: 17685 annotations in available images\n",
      "2024-11-24 15:55:30,992 - INFO - Class Furniture: 11833 annotations in available images\n",
      "2024-11-24 15:55:31,048 - INFO - Class Door: 6462 annotations in available images\n",
      "2024-11-24 15:55:31,106 - INFO - Class Jacket: 9365 annotations in available images\n",
      "2024-11-24 15:55:31,159 - INFO - Class Baked goods: 10075 annotations in available images\n",
      "2024-11-24 15:55:31,214 - INFO - Class Desk: 5663 annotations in available images\n",
      "2024-11-24 15:55:31,268 - INFO - Class Hat: 7109 annotations in available images\n",
      "2024-11-24 15:55:31,327 - INFO - Class Flag: 15650 annotations in available images\n",
      "2024-11-24 15:55:31,383 - INFO - Class Shorts: 11196 annotations in available images\n",
      "2024-11-24 15:55:31,441 - INFO - Class Musical instrument: 14185 annotations in available images\n",
      "2024-11-24 15:55:31,497 - INFO - Class Shirt: 6449 annotations in available images\n",
      "2024-11-24 15:55:31,554 - INFO - Class Camera: 5702 annotations in available images\n",
      "2024-11-24 15:55:31,611 - INFO - Class Beer: 9294 annotations in available images\n",
      "2024-11-24 15:55:31,612 - INFO - Found 25 classes with data in available images\n",
      "2024-11-24 15:55:31,710 - INFO - Class Footwear:\n",
      "2024-11-24 15:55:31,711 - INFO -   Training: 1000 images with 5118 boxes\n",
      "2024-11-24 15:55:31,712 - INFO -   Validation: 500 images with 2782 boxes\n",
      "2024-11-24 15:55:31,775 - INFO - Class Suit:\n",
      "2024-11-24 15:55:31,777 - INFO -   Training: 1000 images with 2468 boxes\n",
      "2024-11-24 15:55:31,778 - INFO -   Validation: 500 images with 1206 boxes\n",
      "2024-11-24 15:55:31,839 - INFO - Class Glasses:\n",
      "2024-11-24 15:55:31,840 - INFO -   Training: 1000 images with 1369 boxes\n",
      "2024-11-24 15:55:31,841 - INFO -   Validation: 500 images with 673 boxes\n",
      "2024-11-24 15:55:31,899 - INFO - Class Dress:\n",
      "2024-11-24 15:55:31,900 - INFO -   Training: 1000 images with 1679 boxes\n",
      "2024-11-24 15:55:31,901 - INFO -   Validation: 500 images with 875 boxes\n",
      "2024-11-24 15:55:31,968 - INFO - Class Jeans:\n",
      "2024-11-24 15:55:31,970 - INFO -   Training: 1000 images with 2446 boxes\n",
      "2024-11-24 15:55:31,971 - INFO -   Validation: 500 images with 1162 boxes\n",
      "2024-11-24 15:55:32,037 - INFO - Class Tire:\n",
      "2024-11-24 15:55:32,039 - INFO -   Training: 1000 images with 4033 boxes\n",
      "2024-11-24 15:55:32,040 - INFO -   Validation: 500 images with 1997 boxes\n",
      "2024-11-24 15:55:32,103 - INFO - Class Fashion accessory:\n",
      "2024-11-24 15:55:32,104 - INFO -   Training: 1000 images with 3927 boxes\n",
      "2024-11-24 15:55:32,105 - INFO -   Validation: 500 images with 1887 boxes\n",
      "2024-11-24 15:55:32,168 - INFO - Class Microphone:\n",
      "2024-11-24 15:55:32,169 - INFO -   Training: 1000 images with 1359 boxes\n",
      "2024-11-24 15:55:32,170 - INFO -   Validation: 500 images with 677 boxes\n",
      "2024-11-24 15:55:32,230 - INFO - Class Guitar:\n",
      "2024-11-24 15:55:32,232 - INFO -   Training: 1000 images with 1422 boxes\n",
      "2024-11-24 15:55:32,232 - INFO -   Validation: 500 images with 736 boxes\n",
      "2024-11-24 15:55:32,297 - INFO - Class Toy:\n",
      "2024-11-24 15:55:32,298 - INFO -   Training: 1000 images with 4207 boxes\n",
      "2024-11-24 15:55:32,299 - INFO -   Validation: 500 images with 1662 boxes\n",
      "2024-11-24 15:55:32,357 - INFO - Class Poster:\n",
      "2024-11-24 15:55:32,358 - INFO -   Training: 1000 images with 1499 boxes\n",
      "2024-11-24 15:55:32,359 - INFO -   Validation: 500 images with 768 boxes\n",
      "2024-11-24 15:55:32,421 - INFO - Class Drink:\n",
      "2024-11-24 15:55:32,423 - INFO -   Training: 1000 images with 2486 boxes\n",
      "2024-11-24 15:55:32,423 - INFO -   Validation: 500 images with 1229 boxes\n",
      "2024-11-24 15:55:32,488 - INFO - Class Bicycle wheel:\n",
      "2024-11-24 15:55:32,489 - INFO -   Training: 1000 images with 3750 boxes\n",
      "2024-11-24 15:55:32,490 - INFO -   Validation: 500 images with 1937 boxes\n",
      "2024-11-24 15:55:32,553 - INFO - Class Furniture:\n",
      "2024-11-24 15:55:32,555 - INFO -   Training: 1000 images with 2583 boxes\n",
      "2024-11-24 15:55:32,556 - INFO -   Validation: 500 images with 1302 boxes\n",
      "2024-11-24 15:55:32,616 - INFO - Class Door:\n",
      "2024-11-24 15:55:32,618 - INFO -   Training: 1000 images with 1455 boxes\n",
      "2024-11-24 15:55:32,619 - INFO -   Validation: 500 images with 708 boxes\n",
      "2024-11-24 15:55:32,677 - INFO - Class Jacket:\n",
      "2024-11-24 15:55:32,678 - INFO -   Training: 1000 images with 2096 boxes\n",
      "2024-11-24 15:55:32,679 - INFO -   Validation: 500 images with 1020 boxes\n",
      "2024-11-24 15:55:32,737 - INFO - Class Baked goods:\n",
      "2024-11-24 15:55:32,739 - INFO -   Training: 1000 images with 2268 boxes\n",
      "2024-11-24 15:55:32,740 - INFO -   Validation: 500 images with 1101 boxes\n",
      "2024-11-24 15:55:32,798 - INFO - Class Desk:\n",
      "2024-11-24 15:55:32,800 - INFO -   Training: 1000 images with 1180 boxes\n",
      "2024-11-24 15:55:32,800 - INFO -   Validation: 500 images with 619 boxes\n",
      "2024-11-24 15:55:32,859 - INFO - Class Hat:\n",
      "2024-11-24 15:55:32,861 - INFO -   Training: 1000 images with 1617 boxes\n",
      "2024-11-24 15:55:32,862 - INFO -   Validation: 500 images with 734 boxes\n",
      "2024-11-24 15:55:32,926 - INFO - Class Flag:\n",
      "2024-11-24 15:55:32,928 - INFO -   Training: 1000 images with 3590 boxes\n",
      "2024-11-24 15:55:32,929 - INFO -   Validation: 500 images with 1786 boxes\n",
      "2024-11-24 15:55:32,992 - INFO - Class Shorts:\n",
      "2024-11-24 15:55:32,993 - INFO -   Training: 1000 images with 2466 boxes\n",
      "2024-11-24 15:55:32,994 - INFO -   Validation: 500 images with 1288 boxes\n",
      "2024-11-24 15:55:33,057 - INFO - Class Musical instrument:\n",
      "2024-11-24 15:55:33,058 - INFO -   Training: 1000 images with 3407 boxes\n",
      "2024-11-24 15:55:33,059 - INFO -   Validation: 500 images with 1555 boxes\n",
      "2024-11-24 15:55:33,120 - INFO - Class Shirt:\n",
      "2024-11-24 15:55:33,122 - INFO -   Training: 1000 images with 1405 boxes\n",
      "2024-11-24 15:55:33,123 - INFO -   Validation: 500 images with 709 boxes\n",
      "2024-11-24 15:55:33,184 - INFO - Class Camera:\n",
      "2024-11-24 15:55:33,185 - INFO -   Training: 1000 images with 1271 boxes\n",
      "2024-11-24 15:55:33,185 - INFO -   Validation: 500 images with 642 boxes\n",
      "2024-11-24 15:55:33,246 - INFO - Class Beer:\n",
      "2024-11-24 15:55:33,247 - INFO -   Training: 1000 images with 1957 boxes\n",
      "2024-11-24 15:55:33,248 - INFO -   Validation: 500 images with 1033 boxes\n",
      "2024-11-24 15:55:33,249 - INFO - \n",
      "Overall Dataset Statistics:\n",
      "2024-11-24 15:55:33,250 - INFO - Total unique training images: 24039\n",
      "2024-11-24 15:55:33,250 - INFO - Total unique validation images: 12500\n",
      "2024-11-24 15:55:33,269 - INFO - Processing 24039 unique images\n",
      "2024-11-24 15:58:48,046 - INFO - Verifying training dataset integrity...\n",
      "2024-11-24 15:58:51,675 - INFO - \n",
      "Bounding box distribution:\n",
      "2024-11-24 15:58:51,677 - INFO - Class 0: 5118 boxes\n",
      "2024-11-24 15:58:51,677 - INFO - Class 1: 2468 boxes\n",
      "2024-11-24 15:58:51,678 - INFO - Class 2: 1369 boxes\n",
      "2024-11-24 15:58:51,679 - INFO - Class 3: 1679 boxes\n",
      "2024-11-24 15:58:51,680 - INFO - Class 4: 2446 boxes\n",
      "2024-11-24 15:58:51,680 - INFO - Class 5: 4033 boxes\n",
      "2024-11-24 15:58:51,681 - INFO - Class 6: 3927 boxes\n",
      "2024-11-24 15:58:51,682 - INFO - Class 7: 1359 boxes\n",
      "2024-11-24 15:58:51,682 - INFO - Class 8: 1422 boxes\n",
      "2024-11-24 15:58:51,683 - INFO - Class 9: 4207 boxes\n",
      "2024-11-24 15:58:51,683 - INFO - Class 10: 1499 boxes\n",
      "2024-11-24 15:58:51,684 - INFO - Class 11: 2486 boxes\n",
      "2024-11-24 15:58:51,684 - INFO - Class 12: 3750 boxes\n",
      "2024-11-24 15:58:51,685 - INFO - Class 13: 2583 boxes\n",
      "2024-11-24 15:58:51,685 - INFO - Class 14: 1455 boxes\n",
      "2024-11-24 15:58:51,686 - INFO - Class 15: 2096 boxes\n",
      "2024-11-24 15:58:51,686 - INFO - Class 16: 2268 boxes\n",
      "2024-11-24 15:58:51,687 - INFO - Class 17: 1180 boxes\n",
      "2024-11-24 15:58:51,688 - INFO - Class 18: 1617 boxes\n",
      "2024-11-24 15:58:51,688 - INFO - Class 19: 3590 boxes\n",
      "2024-11-24 15:58:51,689 - INFO - Class 20: 2466 boxes\n",
      "2024-11-24 15:58:51,690 - INFO - Class 21: 3407 boxes\n",
      "2024-11-24 15:58:51,690 - INFO - Class 22: 1405 boxes\n",
      "2024-11-24 15:58:51,691 - INFO - Class 23: 1271 boxes\n",
      "2024-11-24 15:58:51,691 - INFO - Class 24: 1957 boxes\n",
      "2024-11-24 15:58:51,692 - INFO - Dataset integrity verification passed\n",
      "2024-11-24 15:58:51,709 - INFO - Processing 12500 unique images\n",
      "2024-11-24 15:59:43,752 - INFO - Verifying validation dataset integrity...\n",
      "2024-11-24 15:59:45,515 - INFO - \n",
      "Bounding box distribution:\n",
      "2024-11-24 15:59:45,516 - INFO - Class 0: 2782 boxes\n",
      "2024-11-24 15:59:45,517 - INFO - Class 1: 1206 boxes\n",
      "2024-11-24 15:59:45,518 - INFO - Class 2: 673 boxes\n",
      "2024-11-24 15:59:45,519 - INFO - Class 3: 875 boxes\n",
      "2024-11-24 15:59:45,519 - INFO - Class 4: 1162 boxes\n",
      "2024-11-24 15:59:45,520 - INFO - Class 5: 1997 boxes\n",
      "2024-11-24 15:59:45,521 - INFO - Class 6: 1887 boxes\n",
      "2024-11-24 15:59:45,521 - INFO - Class 7: 677 boxes\n",
      "2024-11-24 15:59:45,522 - INFO - Class 8: 736 boxes\n",
      "2024-11-24 15:59:45,523 - INFO - Class 9: 1662 boxes\n",
      "2024-11-24 15:59:45,523 - INFO - Class 10: 768 boxes\n",
      "2024-11-24 15:59:45,524 - INFO - Class 11: 1229 boxes\n",
      "2024-11-24 15:59:45,524 - INFO - Class 12: 1937 boxes\n",
      "2024-11-24 15:59:45,525 - INFO - Class 13: 1302 boxes\n",
      "2024-11-24 15:59:45,525 - INFO - Class 14: 708 boxes\n",
      "2024-11-24 15:59:45,526 - INFO - Class 15: 1020 boxes\n",
      "2024-11-24 15:59:45,526 - INFO - Class 16: 1101 boxes\n",
      "2024-11-24 15:59:45,527 - INFO - Class 17: 619 boxes\n",
      "2024-11-24 15:59:45,528 - INFO - Class 18: 734 boxes\n",
      "2024-11-24 15:59:45,528 - INFO - Class 19: 1786 boxes\n",
      "2024-11-24 15:59:45,529 - INFO - Class 20: 1288 boxes\n",
      "2024-11-24 15:59:45,529 - INFO - Class 21: 1555 boxes\n",
      "2024-11-24 15:59:45,530 - INFO - Class 22: 709 boxes\n",
      "2024-11-24 15:59:45,530 - INFO - Class 23: 642 boxes\n",
      "2024-11-24 15:59:45,531 - INFO - Class 24: 1033 boxes\n",
      "2024-11-24 15:59:45,532 - INFO - Dataset integrity verification passed\n",
      "2024-11-24 15:59:45,537 - INFO - Created YAML configuration file at E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\dataset.yaml\n",
      "2024-11-24 15:59:45,537 - INFO - \n",
      "Dataset Summary:\n",
      "2024-11-24 15:59:45,538 - INFO - Number of classes: 25\n",
      "2024-11-24 15:59:45,561 - INFO - Training images: 24039\n",
      "2024-11-24 15:59:45,563 - INFO - Validation images: 12500\n",
      "2024-11-24 15:59:45,564 - INFO - \n",
      "Class Distribution:\n",
      "2024-11-24 15:59:50,970 - INFO - Class Footwear: 5118 training boxes, 2782 validation boxes\n",
      "2024-11-24 15:59:56,565 - INFO - Class Suit: 2468 training boxes, 1206 validation boxes\n",
      "2024-11-24 16:00:02,034 - INFO - Class Glasses: 1369 training boxes, 673 validation boxes\n",
      "2024-11-24 16:00:07,563 - INFO - Class Dress: 1679 training boxes, 875 validation boxes\n",
      "2024-11-24 16:00:13,102 - INFO - Class Jeans: 2446 training boxes, 1162 validation boxes\n",
      "2024-11-24 16:00:18,755 - INFO - Class Tire: 4033 training boxes, 1997 validation boxes\n",
      "2024-11-24 16:00:24,287 - INFO - Class Fashion accessory: 3927 training boxes, 1887 validation boxes\n",
      "2024-11-24 16:00:29,753 - INFO - Class Microphone: 1359 training boxes, 677 validation boxes\n",
      "2024-11-24 16:00:35,298 - INFO - Class Guitar: 1422 training boxes, 736 validation boxes\n",
      "2024-11-24 16:00:40,691 - INFO - Class Toy: 4207 training boxes, 1662 validation boxes\n",
      "2024-11-24 16:00:46,121 - INFO - Class Poster: 1499 training boxes, 768 validation boxes\n",
      "2024-11-24 16:00:51,583 - INFO - Class Drink: 2486 training boxes, 1229 validation boxes\n",
      "2024-11-24 16:00:56,994 - INFO - Class Bicycle wheel: 3750 training boxes, 1937 validation boxes\n",
      "2024-11-24 16:01:02,399 - INFO - Class Furniture: 2583 training boxes, 1302 validation boxes\n",
      "2024-11-24 16:01:07,783 - INFO - Class Door: 1455 training boxes, 708 validation boxes\n",
      "2024-11-24 16:01:13,290 - INFO - Class Jacket: 2096 training boxes, 1020 validation boxes\n",
      "2024-11-24 16:01:18,670 - INFO - Class Baked goods: 2268 training boxes, 1101 validation boxes\n",
      "2024-11-24 16:01:24,077 - INFO - Class Desk: 1180 training boxes, 619 validation boxes\n",
      "2024-11-24 16:01:29,492 - INFO - Class Hat: 1617 training boxes, 734 validation boxes\n",
      "2024-11-24 16:01:34,939 - INFO - Class Flag: 3590 training boxes, 1786 validation boxes\n",
      "2024-11-24 16:01:40,363 - INFO - Class Shorts: 2466 training boxes, 1288 validation boxes\n",
      "2024-11-24 16:01:45,714 - INFO - Class Musical instrument: 3407 training boxes, 1555 validation boxes\n",
      "2024-11-24 16:01:51,197 - INFO - Class Shirt: 1405 training boxes, 709 validation boxes\n",
      "2024-11-24 16:01:56,614 - INFO - Class Camera: 1271 training boxes, 642 validation boxes\n",
      "2024-11-24 16:02:01,972 - INFO - Class Beer: 1957 training boxes, 1033 validation boxes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.36 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.23  Python-3.10.0 torch-2.5.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3080, 10240MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\dataset.yaml, epochs=300, time=None, patience=20, batch=16, imgsz=640, save=True, save_period=10, cache=False, device=0, workers=6, project=E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530, name=yolo_boxes_1000, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\yolo_boxes_1000\n",
      "Overriding model.yaml nc=80 with nc=25\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2125723  ultralytics.nn.modules.head.Detect           [25, [128, 256, 512]]         \n",
      "Model summary: 225 layers, 11,145,275 parameters, 11,145,259 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\yolo_boxes_1000', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\dataset\\train\\images\\4512a4e6351696f6.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\dataset\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\dataset\\val\\labels.cache\n",
      "Plotting labels to E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\yolo_boxes_1000\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 6 dataloader workers\n",
      "Logging results to \u001b[1mE:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\yolo_boxes_1000\u001b[0m\n",
      "Starting training for 300 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.339      0.372      0.284       0.17\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.326      0.352      0.257      0.145\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.243      0.261      0.165     0.0846\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.332      0.292      0.212      0.115\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.377      0.317      0.245      0.137\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.351      0.343      0.262      0.148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088       0.33      0.367      0.283      0.164\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.364      0.369      0.294      0.171\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.389      0.381      0.307      0.183\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.361      0.399       0.31      0.183\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.378      0.415      0.325      0.196\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.396      0.405      0.328      0.197\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.388      0.419      0.334      0.201\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.387      0.436      0.347      0.211\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.393      0.437      0.347      0.211\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088        0.4      0.435      0.357       0.22\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088        0.4      0.438       0.36      0.221\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.403      0.439      0.366      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.413      0.449      0.372      0.231\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.415      0.457      0.376      0.233\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088       0.41      0.458      0.377      0.235\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.429      0.446       0.38      0.238\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.414      0.465      0.386      0.241\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.417      0.466      0.386      0.241\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.431      0.462      0.389      0.244\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.415      0.473       0.39      0.245\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.428       0.46      0.391      0.246\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088       0.43       0.47      0.395      0.249\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.426      0.474      0.397       0.25\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.421      0.483      0.397      0.251\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.425      0.482      0.401      0.253\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.429      0.475      0.401      0.254\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.431      0.474        0.4      0.253\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.427      0.478      0.401      0.254\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.426      0.481      0.403      0.255\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.433      0.475      0.404      0.256\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.431       0.48      0.405      0.257\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.426      0.484      0.405      0.257\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.423      0.487      0.405      0.258\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.429      0.483      0.405      0.258\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.428      0.485      0.405      0.258\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.425      0.487      0.406      0.258\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.424      0.488      0.406      0.259\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.423      0.489      0.407      0.259\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.421       0.49      0.407      0.259\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088       0.42      0.493      0.408       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.423       0.49      0.408       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.424      0.491      0.408       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.421      0.493      0.408       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088       0.42      0.494      0.408       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.423      0.492      0.408       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088       0.42      0.495      0.408       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.421      0.494      0.408       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.421      0.495      0.408       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.423      0.495      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.424      0.494      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.424      0.494      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.422      0.496      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.421      0.495      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.422      0.495      0.408      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088       0.42      0.497      0.408      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.421      0.496      0.408      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.421      0.497      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.421      0.496      0.408      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088       0.42      0.498      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.419        0.5      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.419        0.5      0.408      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.419      0.501      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.421      0.499      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.419        0.5      0.408      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.421        0.5      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.422      0.499      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.422      0.499      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.422        0.5      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.424      0.499      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.425      0.497      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.424      0.498      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.424      0.498      0.409      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.424      0.498      0.408      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.425      0.497      0.408      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.424      0.498      0.408      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.424      0.499      0.408      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.424      0.498      0.407      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.426      0.497      0.407      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.427      0.496      0.407      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.426      0.498      0.407      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.426      0.497      0.407      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.426      0.497      0.407       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.426      0.496      0.407       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.427      0.494      0.407       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.428      0.494      0.406       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.425      0.497      0.406       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.424      0.498      0.406       0.26\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "                   all      12500      30088      0.425      0.496      0.406       0.26\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 20 epochs. Best results observed at epoch 74, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=20) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "94 epochs completed in 10.275 hours.\n",
      "Optimizer stripped from E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\yolo_boxes_1000\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\yolo_boxes_1000\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating E:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\yolo_boxes_1000\\weights\\best.pt...\n",
      "Ultralytics 8.3.23  Python-3.10.0 torch-2.5.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3080, 10240MiB)\n",
      "Model summary (fused): 168 layers, 11,135,259 parameters, 0 gradients, 28.5 GFLOPs\n",
      "                   all      12500      30088      0.423      0.499      0.409      0.261\n",
      "              Footwear        500       2782      0.246     0.0257      0.117     0.0484\n",
      "                  Suit        500       1206      0.418        0.6      0.427      0.268\n",
      "               Glasses        500        673      0.468      0.447      0.393      0.206\n",
      "                 Dress        500        875      0.394      0.621      0.412      0.288\n",
      "                 Jeans        500       1162      0.287      0.446       0.26      0.156\n",
      "                  Tire        500       1997      0.487      0.424      0.423      0.251\n",
      "     Fashion accessory        500       1887      0.311     0.0922     0.0911     0.0477\n",
      "            Microphone        500        677      0.421      0.448      0.334      0.166\n",
      "                Guitar        500        736      0.564      0.723      0.571      0.371\n",
      "                   Toy        500       1662      0.432      0.403      0.353      0.206\n",
      "                Poster        500        768      0.476      0.656      0.593      0.498\n",
      "                 Drink        500       1229      0.248      0.187      0.147     0.0774\n",
      "         Bicycle wheel        500       1937      0.571      0.681      0.577       0.37\n",
      "             Furniture        500       1302      0.256      0.142      0.114      0.066\n",
      "                  Door        500        708      0.407      0.568      0.487      0.294\n",
      "                Jacket        500       1020      0.399      0.618      0.445      0.271\n",
      "           Baked goods        500       1101      0.476      0.606      0.502       0.37\n",
      "                  Desk        500        619      0.436      0.625      0.537       0.31\n",
      "                   Hat        500        734      0.396      0.698      0.461      0.363\n",
      "                  Flag        500       1786      0.569      0.602      0.566      0.318\n",
      "                Shorts        500       1288      0.465      0.637      0.472      0.262\n",
      "    Musical instrument        500       1555      0.357      0.158      0.162     0.0719\n",
      "                 Shirt        500        709      0.396      0.671      0.512      0.406\n",
      "                Camera        500        642      0.616       0.78      0.724      0.501\n",
      "                  Beer        500       1033      0.465      0.628      0.542      0.351\n",
      "Speed: 0.2ms preprocess, 1.5ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Data 255\\YOLO multi-class\\yolo_training\\multi_class_run_1000_images_20241124_155530\\yolo_boxes_1000\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 02:22:35,778 - ERROR - Failed to save training results: 'DetMetrics' object has no attribute 'epoch'. See valid attributes below.\n",
      "\n",
      "    Utility class for computing detection metrics such as precision, recall, and mean average precision (mAP) of an\n",
      "    object detection model.\n",
      "\n",
      "    Args:\n",
      "        save_dir (Path): A path to the directory where the output plots will be saved. Defaults to current directory.\n",
      "        plot (bool): A flag that indicates whether to plot precision-recall curves for each class. Defaults to False.\n",
      "        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n",
      "        names (dict of str): A dict of strings that represents the names of the classes. Defaults to an empty tuple.\n",
      "\n",
      "    Attributes:\n",
      "        save_dir (Path): A path to the directory where the output plots will be saved.\n",
      "        plot (bool): A flag that indicates whether to plot the precision-recall curves for each class.\n",
      "        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n",
      "        names (dict of str): A dict of strings that represents the names of the classes.\n",
      "        box (Metric): An instance of the Metric class for storing the results of the detection metrics.\n",
      "        speed (dict): A dictionary for storing the execution time of different parts of the detection process.\n",
      "\n",
      "    Methods:\n",
      "        process(tp, conf, pred_cls, target_cls): Updates the metric results with the latest batch of predictions.\n",
      "        keys: Returns a list of keys for accessing the computed detection metrics.\n",
      "        mean_results: Returns a list of mean values for the computed detection metrics.\n",
      "        class_result(i): Returns a list of values for the computed detection metrics for a specific class.\n",
      "        maps: Returns a dictionary of mean average precision (mAP) values for different IoU thresholds.\n",
      "        fitness: Computes the fitness score based on the computed detection metrics.\n",
      "        ap_class_index: Returns a list of class indices sorted by their average precision (AP) values.\n",
      "        results_dict: Returns a dictionary that maps detection metric keys to their computed values.\n",
      "        curves: TODO\n",
      "        curves_results: TODO\n",
      "    \n",
      "2024-11-25 02:22:35,788 - ERROR - Error during training with 1000 images per class: 'list' object is not callable\n",
      "2024-11-25 02:22:35,790 - INFO - All training complete!\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "import io\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize paths and parameters\n",
    "    image_dir = r'E:\\Data 255\\YOLO multi-class\\image_data\\training_images_25_class'\n",
    "    project_dir = r'E:\\Data 255\\YOLO multi-class\\yolo_training'\n",
    "    class_desc_path = 'oidv7-class-descriptions-boxable.csv'\n",
    "    annotations_path = 'oidv6-train-annotations-bbox.csv'\n",
    "    \n",
    "    # Create output file path\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = os.path.join(project_dir, f'notebook_output_{timestamp}.txt')\n",
    "    \n",
    "    # Capture all output\n",
    "    output_buffer = io.StringIO()\n",
    "    with redirect_stdout(output_buffer), redirect_stderr(output_buffer):\n",
    "        # Your existing code here\n",
    "        set_all_seeds()\n",
    "        \n",
    "        # Create project directory\n",
    "        os.makedirs(project_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize class loader\n",
    "        class_loader = MultiClassDataLoader(class_desc_path, annotations_path, image_dir)\n",
    "        \n",
    "        # Training parameters\n",
    "        image_counts = [500,1000]  # Images per class\n",
    "        fixed_val_size = 500  # Validation images per class\n",
    "        \n",
    "        # Train models\n",
    "        results = []\n",
    "        for image_count in image_counts:\n",
    "            try:\n",
    "                metrics = train_multi_class_yolo(\n",
    "                    image_count,  # Now represents images per class\n",
    "                    image_dir,\n",
    "                    project_dir,\n",
    "                    class_loader,\n",
    "                    fixed_val_size  # Now represents images per class\n",
    "                )\n",
    "                if metrics:\n",
    "                    results.append(metrics)\n",
    "                    save_results(results, project_dir)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during training with {image_count} images per class: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        logging.info(\"All training complete!\")\n",
    "    \n",
    "    # Save captured output\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(output_buffer.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
